{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23825a4a-8cbb-4fe6-855e-0f0108a7400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.pminterviewprep.club\n",
      "Crawling: https://www.pminterviewprep.club/challenge/\n",
      "Crawling: https://www.pminterviewprep.club/learn/\n",
      "Crawling: https://www.pminterviewprep.club/jobs/\n",
      "Crawling: https://www.pminterviewprep.club/resume/\n",
      "Crawling: https://www.pminterviewprep.club/portfolio/\n",
      "Crawling: https://www.pminterviewprep.club/community/\n",
      "Crawling: https://www.pminterviewprep.club/mentorship/\n",
      "Crawling: https://www.pminterviewprep.club/\n",
      "Crawling: https://www.pminterviewprep.club/blog/\n",
      "Crawling: https://www.pminterviewprep.club/resources/\n",
      "Crawling: https://www.pminterviewprep.club/faq/\n",
      "Crawling: https://www.pminterviewprep.club/careers/\n",
      "Crawling: https://www.pminterviewprep.club/subscription/\n",
      "Crawling: https://www.pminterviewprep.club/refund-policy/\n",
      "Crawling: https://www.pminterviewprep.club/career-clarity/\n",
      "Crawling: https://www.pminterviewprep.club/guides/\n",
      "Crawling: https://www.pminterviewprep.club/guides/how-to-get-hired-at-an-indian-startup-the-inside-scoop/\n",
      "Crawling: https://www.pminterviewprep.club/guides/product-case-studies-lessons-from-lesser-known-products/\n",
      "Crawling: https://www.pminterviewprep.club/guides/how-to-answer-commonly-asked-questions/\n",
      "Crawling: https://www.pminterviewprep.club/resources/apmrpm-programs-complete-guide-for-graduatespost-graduates/\n",
      "Crawling: https://www.pminterviewprep.club/resources/pm-job-search-highresponse-outreach-pack/\n",
      "Crawling: https://www.pminterviewprep.club/resources/the-30-day-70-minute-method-to-master-product-manager-interviews/\n",
      "Crawling: https://www.pminterviewprep.club/blog/getting-hired-at-an-indian-startup-mastering-outreach-the-first-call/\n",
      "Crawling: https://www.pminterviewprep.club/blog/getting-hired-at-an-indian-startup-finding-pitching-the-right-opportunities/\n",
      "Crawling: https://www.pminterviewprep.club/blog/build-your-product-muscle-a-3-step-workout-for-aspiring-pms/\n",
      "Crawling: https://www.pminterviewprep.club/blog/blog-2-getting-hired-at-an-indian-startup-16-ways-to-stand-out-and-why-most-people-wont-try-them/\n",
      "Crawling: https://www.pminterviewprep.club/blog/when-ai-turns-into-anchors-the-450-million-meltdown-of-builderai/\n",
      "Crawling: https://www.pminterviewprep.club/blog/uber-indias-quiet-reboot-how-feature-tweaks-became-profit-levers/\n",
      "Crawling: https://www.pminterviewprep.club/blog/getting-hired-at-an-indian-startup-why-the-old-rules-dont-apply/\n",
      "\n",
      "Total chunks: 67\n",
      "\n",
      "URL: https://www.pminterviewprep.club\n",
      "CHUNK: Practice real Interview Questions. Get AI feedback. Get that Job!\n",
      "10 free AI credits • No card required\n",
      "Everything You Need to Succeed\n",
      "Actual Interview Questions with AI feedback and Expert Answers\n",
      "Common frameworks and strategies\n",
      "Build your product case studies\n",
      "Choose from guesstimate, design, or strategy questions.\n",
      "Use our framework or create your own approach.\n",
      "Receive instant AI analysis and improvement tips.\n",
      "- Free Community Access & Support\n",
      "- 3000+ actual Interview Questions with Expert Ans\n",
      "--------------------------------------------------------------------------------\n",
      "URL: https://www.pminterviewprep.club\n",
      "CHUNK: \"PM Interview Prep Club has been really helpful in my preparation. The practice content is practical and close to real interviews, the AI resume review gave me clear feedback I could act on, and the community makes the whole process a lot easier and less lonely.\"– Sabyasachi Mishra\n",
      "\"PM interview prep club has been really helpful to put structure to my thought while practicing. The questions asked in interviews and in the practice sheet are very similar or same most of the times. Hanshika herself\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "BASE_URL = \"https://www.pminterviewprep.club\"  ## Pass the website of whom you wish you train the model on. In this version, I am scraping from the website. However, we can also train the model via various knowledge sources including PDFs, FAQs etc.,\n",
    "MAX_PAGES = 30\n",
    "\n",
    "CHUNK_SIZE = 500     # tokens\n",
    "OVERLAP = 100        # tokens\n",
    "ENCODING = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LINK FILTERING\n",
    "# =========================\n",
    "def is_valid_internal_link(link: str) -> bool:\n",
    "    if not link:\n",
    "        return False\n",
    "\n",
    "    parsed = urlparse(link)\n",
    "    base_netloc = urlparse(BASE_URL).netloc\n",
    "\n",
    "    if parsed.netloc and parsed.netloc != base_netloc:\n",
    "        return False\n",
    "\n",
    "    skip_keywords = [\n",
    "        \"login\", \"signup\", \"dashboard\",\n",
    "        \"privacy\", \"terms\", \"auth\",\n",
    "        \"#\"\n",
    "    ]\n",
    "\n",
    "    return not any(k in link.lower() for k in skip_keywords)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PAGE FETCH\n",
    "# =========================\n",
    "async def fetch_page(page, url: str) -> str | None:\n",
    "    try:\n",
    "        await page.goto(url, timeout=20000)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        return await page.content()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CLEAN TEXT EXTRACTION\n",
    "# =========================\n",
    "def extract_clean_text(html: str) -> str:\n",
    "    # --- Primary: Trafilatura (best for blogs/guides) ---\n",
    "    text = trafilatura.extract(\n",
    "        html,\n",
    "        favor_recall=True,\n",
    "        include_tables=True,\n",
    "        include_comments=False\n",
    "    )\n",
    "\n",
    "    if text and len(text) > 400:\n",
    "        return post_process(text)\n",
    "\n",
    "    # --- Fallback: BeautifulSoup (marketing pages) ---\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for tag in soup([\n",
    "        \"script\", \"style\", \"noscript\",\n",
    "        \"header\", \"footer\", \"nav\",\n",
    "        \"aside\", \"form\", \"button\",\n",
    "        \"svg\", \"iframe\"\n",
    "    ]):\n",
    "        tag.decompose()\n",
    "\n",
    "    for tag in soup.find_all(\n",
    "        attrs={\n",
    "            \"class\": re.compile(\n",
    "                r\"(nav|footer|header|menu|cookie|banner|cta|subscribe|pricing|popup)\",\n",
    "                re.I\n",
    "            )\n",
    "        }\n",
    "    ):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    return post_process(text)\n",
    "\n",
    "\n",
    "def post_process(text: str) -> str:\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if len(line) < 30:\n",
    "            continue\n",
    "        if re.search(\n",
    "            r\"(sign up|subscribe|get started|join now|buy now|book a call)\",\n",
    "            line,\n",
    "            re.I\n",
    "        ):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    # Deduplicate consecutive lines\n",
    "    deduped = []\n",
    "    prev = None\n",
    "    for line in cleaned_lines:\n",
    "        if line != prev:\n",
    "            deduped.append(line)\n",
    "        prev = line\n",
    "\n",
    "    return \"\\n\".join(deduped).strip()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TOKEN-BASED CHUNKING\n",
    "# =========================\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = OVERLAP\n",
    ") -> List[str]:\n",
    "    tokens = ENCODING.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    total = len(tokens)\n",
    "\n",
    "    while start < total:\n",
    "        end = start + chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk = ENCODING.decode(chunk_tokens).strip()\n",
    "\n",
    "        if len(chunk) > 200:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CRAWLER (RETURNS CHUNKS)\n",
    "# =========================\n",
    "async def crawl_site() -> List[Dict]:\n",
    "    visited = set()\n",
    "    to_visit = [BASE_URL]\n",
    "    chunked_results: List[Dict] = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        while to_visit and len(visited) < MAX_PAGES:\n",
    "            url = to_visit.pop(0)\n",
    "\n",
    "            if url in visited:\n",
    "                continue\n",
    "\n",
    "            print(f\"Crawling: {url}\")\n",
    "            visited.add(url)\n",
    "\n",
    "            html = await fetch_page(page, url)\n",
    "            if not html:\n",
    "                continue\n",
    "\n",
    "            text = extract_clean_text(html)\n",
    "            if not text or len(text) < 500:\n",
    "                continue\n",
    "\n",
    "            chunks = chunk_text(text)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_results.append({\n",
    "                    \"url\": url,\n",
    "                    \"chunk_id\": i,\n",
    "                    \"text\": chunk\n",
    "                })\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                link = urljoin(url, a[\"href\"]).split(\"?\")[0]\n",
    "                if is_valid_internal_link(link) and link not in visited:\n",
    "                    to_visit.append(link)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return chunked_results\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN (SCRIPT SAFE)\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Use this only when running as a script:\n",
    "    results = await crawl_site()\n",
    "\n",
    "    print(f\"\\nTotal chunks: {len(results)}\\n\")\n",
    "\n",
    "    # Sanity check\n",
    "    for r in results[:2]:\n",
    "        print(\"URL:\", r[\"url\"])\n",
    "        print(\"CHUNK:\", r[\"text\"][:500])\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123390d6-1b5a-4b5e-b86e-75eb7d7d57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = ## Pass your Google gemini API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d23423-77e8-42f8-b263-44ff01a9674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Dict\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "EMBEDDING_DIM = 768  # Gemini embedding size\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.index = faiss.IndexFlatL2(EMBEDDING_DIM)\n",
    "        self.metadata: List[Dict] = []\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            res = genai.embed_content(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=text,\n",
    "                task_type=\"retrieval_document\"\n",
    "            )\n",
    "            embeddings.append(res[\"embedding\"])\n",
    "        return embeddings\n",
    "\n",
    "    def add_chunks(self, chunks: List[Dict]):\n",
    "        texts = [c[\"text\"] for c in chunks]\n",
    "        vectors = np.array(self.embed_texts(texts)).astype(\"float32\")\n",
    "        self.index.add(vectors)\n",
    "        self.metadata.extend(chunks)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        q = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=query,\n",
    "            task_type=\"retrieval_query\"\n",
    "        )[\"embedding\"]\n",
    "\n",
    "        q_vec = np.array([q]).astype(\"float32\")\n",
    "        _, idxs = self.index.search(q_vec, top_k)\n",
    "        return [self.metadata[i] for i in idxs[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54318358-254b-4e15-b937-53854e7f3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "def answer_with_gemini(query: str, context_chunks):\n",
    "    context = \"\\n\\n\".join([c[\"text\"] for c in context_chunks])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a website assistant.\n",
    "Answer ONLY from the context below.\n",
    "If the answer is not present, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "## You can change the prompt as required. I have used a very basic prompt\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe9e54b-3da9-47cb-a7fa-44259198af9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, many parts of the context are relevant for a senior product manager.\n",
      "\n",
      "The context mentions:\n",
      "*   \"with 7 years of experience submitted.\"\n",
      "*   Connecting with \"experienced Product Leaders from top tech companies.\"\n",
      "*   \"1:1 sessions tailored to your goals and prep stage\" and building a \"career roadmap that fits your story and aspirations.\"\n",
      "*   Strategies for applying to companies, such as writing a Product Strategy Memo, which \"secured a call directly with the founders\" for a mentee.\n",
      "*   The \"How to Be Valuable\" framework (Acquire New Users, Increase Engagement/Retention, Improve Unit Economics) is relevant for strategic thinking at all levels.\n",
      "*   Guidance on \"Mastering Outreach & The First Call,\" \"Finding & Pitching the Right Opportunities,\" and \"16 Ways to Stand Out.\"\n"
     ]
    }
   ],
   "source": [
    "store = VectorStore()\n",
    "store.add_chunks(results)   # results from your crawler\n",
    "\n",
    "query = \"I a senior product manager. Is this relevant for me?\"   ## Pass your query here\n",
    "\n",
    "hits = store.search(query)\n",
    "print(answer_with_gemini(query, hits))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdebf3-225b-46ff-87cd-5159499aaa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
